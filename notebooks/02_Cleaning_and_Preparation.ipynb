{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 02_Cleaning_and_Preparation — Splits, PDFs→Imágenes, Preprocesado y OCR\n",
        "\n",
        "Objetivos:\n",
        "- Generar splits `train/val/test` (70/15/15) por `doc_id` y guardar en `data/splits/`.\n",
        "- Convertir PDFs a imágenes a 300 dpi en `data/raw/` como `<doc_id>_p01.png`, etc.\n",
        "- Preprocesar imágenes (deskew, Otsu, CLAHE) y guardar opcionalmente en `data/processed/`.\n",
        "- Ejecutar OCR base (PaddleOCR o Tesseract) y guardar `data/ocr/<doc_id>.json` con líneas, palabras y bboxes (para LayoutLMv3).\n",
        "- Validar consistencia básica de GT.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import io\n",
        "import cv2\n",
        "import json\n",
        "import glob\n",
        "import time\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "\n",
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "ROOT = Path(os.getcwd())\n",
        "# Normalizar raíz para evitar notebooks/data si se ejecuta desde notebooks/\n",
        "if (ROOT.name == 'notebooks') and (ROOT.parent / 'data').exists():\n",
        "    ROOT = ROOT.parent\n",
        "else:\n",
        "    for p in [ROOT] + list(ROOT.parents):\n",
        "        if (p / 'data').exists():\n",
        "            ROOT = p\n",
        "            break\n",
        "DATA = ROOT / 'data'\n",
        "RAW = DATA / 'raw'\n",
        "PROC = DATA / 'processed'\n",
        "OCR_DIR = DATA / 'ocr'\n",
        "SPLITS = DATA / 'splits'\n",
        "GT_DIR = DATA / 'gt'\n",
        "MANIFEST = DATA / 'manifest.csv'\n",
        "\n",
        "SPLITS.mkdir(parents=True, exist_ok=True)\n",
        "PROC.mkdir(parents=True, exist_ok=True)\n",
        "OCR_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Utilidades\n",
        "\n",
        "def load_manifest():\n",
        "    return pd.read_csv(MANIFEST)\n",
        "\n",
        "def save_splits(doc_ids, train_ratio=0.7, val_ratio=0.15):\n",
        "    rng = np.random.RandomState(42)\n",
        "    doc_ids = list(sorted(doc_ids))\n",
        "    rng.shuffle(doc_ids)\n",
        "    n = len(doc_ids)\n",
        "    n_train = int(n*train_ratio)\n",
        "    n_val = int(n*val_ratio)\n",
        "    train = doc_ids[:n_train]\n",
        "    val = doc_ids[n_train:n_train+n_val]\n",
        "    test = doc_ids[n_train+n_val:]\n",
        "    for name, split in [('train',train),('val',val),('test',test)]:\n",
        "        (SPLITS/f'{name}.txt').write_text('\\n'.join(split), encoding='utf-8')\n",
        "    return train, val, test\n",
        "\n",
        "# PDF → Imágenes (300 dpi)\n",
        "\n",
        "def pdf_to_images(pdf_path: Path, out_prefix: str, dpi=300):\n",
        "    pages = []\n",
        "    try:\n",
        "        from pdf2image import convert_from_path\n",
        "        pages = convert_from_path(str(pdf_path), dpi=dpi)\n",
        "    except Exception:\n",
        "        try:\n",
        "            import fitz  # PyMuPDF\n",
        "            doc = fitz.open(str(pdf_path))\n",
        "            for i, page in enumerate(doc):\n",
        "                pix = page.get_pixmap(dpi=dpi)\n",
        "                img = Image.frombytes('RGB', [pix.width, pix.height], pix.samples)\n",
        "                pages.append(img)\n",
        "        except Exception as e2:\n",
        "            print(f'[WARN] No se pudo convertir PDF {pdf_path}: {e2}')\n",
        "            return []\n",
        "    out_paths = []\n",
        "    for i, img in enumerate(pages, start=1):\n",
        "        out_path = RAW / f'{out_prefix}_p{i:02d}.png'\n",
        "        img.save(out_path)\n",
        "        out_paths.append(out_path)\n",
        "    return out_paths\n",
        "\n",
        "# Preprocesado: deskew, Otsu, CLAHE\n",
        "\n",
        "def deskew(image):\n",
        "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "    coords = np.column_stack(np.where(gray > 0))\n",
        "    angle = 0.0\n",
        "    if coords.size > 0:\n",
        "        rect = cv2.minAreaRect(coords)\n",
        "        angle = rect[-1]\n",
        "        if angle < -45:\n",
        "            angle = -(90 + angle)\n",
        "        else:\n",
        "            angle = -angle\n",
        "    (h, w) = image.shape[:2]\n",
        "    center = (w // 2, h // 2)\n",
        "    M = cv2.getRotationMatrix2D(center, angle, 1.0)\n",
        "    rotated = cv2.warpAffine(image, M, (w, h), flags=cv2.INTER_CUBIC, borderMode=cv2.BORDER_REPLICATE)\n",
        "    return rotated\n",
        "\n",
        "clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
        "\n",
        "def preprocess_image(path: Path):\n",
        "    img = cv2.imread(str(path))\n",
        "    if img is None:\n",
        "        return None\n",
        "    img = deskew(img)\n",
        "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "    gray = clahe.apply(gray)\n",
        "    _, th = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
        "    out = cv2.cvtColor(th, cv2.COLOR_GRAY2BGR)\n",
        "    out_path = PROC / Path(path).name\n",
        "    cv2.imwrite(str(out_path), out)\n",
        "    return out_path\n",
        "\n",
        "# OCR base → JSON compatible con LayoutLMv3 (palabras y bboxes normalizados 0-1000)\n",
        "\n",
        "def ocr_document_images(doc_id: str, page_paths: list):\n",
        "    # Intenta PaddleOCR, si no usa Tesseract\n",
        "    use_paddle = False\n",
        "    ocr = None\n",
        "    try:\n",
        "        from paddleocr import PaddleOCR\n",
        "        ocr = PaddleOCR(lang='es', use_angle_cls=True, show_log=False)\n",
        "        use_paddle = True\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    results = []\n",
        "    for pth in page_paths:\n",
        "        img = cv2.imread(str(pth))\n",
        "        if img is None:\n",
        "            continue\n",
        "        h, w = img.shape[:2]\n",
        "        if use_paddle and ocr is not None:\n",
        "            res = ocr.ocr(str(pth), cls=True)\n",
        "            words = []\n",
        "            lines = []\n",
        "            for block in res:\n",
        "                for box, (text, conf) in block:\n",
        "                    xs = [pt[0] for pt in box]\n",
        "                    ys = [pt[1] for pt in box]\n",
        "                    x1, y1, x2, y2 = min(xs), min(ys), max(xs), max(ys)\n",
        "                    bbox = [int(1000*x1/w), int(1000*y1/h), int(1000*x2/w), int(1000*y2/h)]\n",
        "                    words.append({'text': text, 'bbox': bbox, 'conf': float(conf)})\n",
        "                line_text = ' '.join([w['text'] for w in words])\n",
        "                lines.append({'text': line_text})\n",
        "            results.append({'page_path': str(pth), 'width': w, 'height': h, 'words': words, 'lines': lines})\n",
        "        else:\n",
        "            try:\n",
        "                import pytesseract\n",
        "                data = pytesseract.image_to_data(img, output_type=pytesseract.Output.DICT, lang='spa')\n",
        "                words = []\n",
        "                lines_map = {}\n",
        "                for i in range(len(data['text'])):\n",
        "                    txt = data['text'][i]\n",
        "                    conf = data.get('conf',[\"0\"]) [i]\n",
        "                    if not txt or str(txt).strip()=='' or txt=='-1':\n",
        "                        continue\n",
        "                    x, y, bw, bh = data['left'][i], data['top'][i], data['width'][i], data['height'][i]\n",
        "                    bbox = [int(1000*x/w), int(1000*y/h), int(1000*(x+bw)/w), int(1000*(y+bh)/h)]\n",
        "                    words.append({'text': txt, 'bbox': bbox, 'conf': float(conf) if str(conf).replace('.','',1).isdigit() else 0.0})\n",
        "                    line_num = data.get('line_num',[1])[i]\n",
        "                    lines_map.setdefault(line_num, []).append(txt)\n",
        "                lines = [{'text': ' '.join(v)} for _, v in sorted(lines_map.items())]\n",
        "                results.append({'page_path': str(pth), 'width': w, 'height': h, 'words': words, 'lines': lines})\n",
        "            except Exception as e2:\n",
        "                print('[WARN] OCR falló en', pth, e2)\n",
        "    out_path = OCR_DIR / f'{doc_id}.json'\n",
        "    with open(out_path, 'w', encoding='utf-8') as f:\n",
        "        json.dump({'doc_id': doc_id, 'pages': results}, f, ensure_ascii=False)\n",
        "    return out_path\n",
        "\n",
        "# Validaciones básicas de GT\n",
        "\n",
        "def basic_gt_validation(gt_obj: dict):\n",
        "    campos = gt_obj.get('campos',{})\n",
        "    iva_pct = campos.get('iva_porcentaje')\n",
        "    iva_val = campos.get('iva_valor')\n",
        "    if iva_pct in [None, '', 0, 0.0]:\n",
        "        if isinstance(iva_val, (int, float)) and abs(float(iva_val)) > 0:\n",
        "            return {'warning': 'iva_porcentaje=0 pero iva_valor>0'}\n",
        "    return {}\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Splits ya existen\n",
            "PDF factura_0101.pdf -> 1 páginas\n",
            "PDF factura_0102.pdf -> 1 páginas\n",
            "PDF factura_0103.pdf -> 2 páginas\n",
            "Imágenes preprocesadas: 104\n",
            "Documentos con OCR generado (nuevos): 0\n",
            "Advertencias GT: 0\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'OK'"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Flujo principal: splits, conversión de PDFs, preprocesado y OCR\n",
        "\n",
        "manifest = load_manifest()\n",
        "\n",
        "# Generar splits si no existen\n",
        "if not (SPLITS/'train.txt').exists():\n",
        "    train, val, test = save_splits(manifest['doc_id'].tolist(), 0.7, 0.15)\n",
        "    print('Splits generados:', len(train), len(val), len(test))\n",
        "else:\n",
        "    print('Splits ya existen')\n",
        "\n",
        "# Convertir PDFs a imágenes\n",
        "pdfs = [p for p in (RAW.glob('*.pdf'))]\n",
        "for pdf in pdfs:\n",
        "    base_id = pdf.stem\n",
        "    out_paths = pdf_to_images(pdf, base_id, dpi=300)\n",
        "    if out_paths:\n",
        "        print(f'PDF {pdf.name} -> {len(out_paths)} páginas')\n",
        "\n",
        "# Preprocesar imágenes (opcional)\n",
        "img_paths = list(RAW.glob('*.png')) + list(RAW.glob('*.jpg')) + list(RAW.glob('*.jpeg'))\n",
        "processed = 0\n",
        "for p in img_paths:\n",
        "    pp = preprocess_image(p)\n",
        "    if pp is not None:\n",
        "        processed += 1\n",
        "print('Imágenes preprocesadas:', processed)\n",
        "\n",
        "# Construir lista de páginas por doc_id (considerando multipágina)\n",
        "doc_to_pages = {}\n",
        "for p in sorted(list(RAW.glob('*.png')) + list(RAW.glob('*.jpg')) + list(RAW.glob('*.jpeg'))):\n",
        "    base = re.sub(r'_p\\d+$', '', p.stem)\n",
        "    doc_to_pages.setdefault(base, []).append(p)\n",
        "\n",
        "# OCR por documento\n",
        "count_ocr = 0\n",
        "for doc_id, pages in doc_to_pages.items():\n",
        "    out = OCR_DIR / f'{doc_id}.json'\n",
        "    if out.exists():\n",
        "        continue\n",
        "    ocr_document_images(doc_id, pages)\n",
        "    count_ocr += 1\n",
        "print('Documentos con OCR generado (nuevos):', count_ocr)\n",
        "\n",
        "# Validación simple GT\n",
        "bad = []\n",
        "for p in GT_DIR.glob('*.json'):\n",
        "    with open(p,'r',encoding='utf-8') as f:\n",
        "        gt = json.load(f)\n",
        "    v = basic_gt_validation(gt)\n",
        "    if v:\n",
        "        bad.append({'gt': str(p), **v})\n",
        "\n",
        "print('Advertencias GT:', len(bad))\n",
        "pd.DataFrame(bad).head(10) if bad else 'OK'\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
